# chatGPT
[你觉得最近大热的 chatGPT 会取代你的工作吗？](https://www.zhihu.com/question/582834721/answer/2887416267)

> Author: #NellNell
> Last update: [编辑于 2023-02-11]
> Link:
> Tag:
> 沙海拾金:

大语言模型和人类自然语言，有一个天然的差异，就是**学习机制**不同，这就决定了可应用领域和可替代性质的不同。

至少目前看来，大语言模型还是通过语料数据的相关性统计来进行学习和推演。

也就是给它一大段话，它关注的是语词之间**统计学上的相似性**，而非**逻辑关系**。比如“团结”和“力量”一起出现的频率较高，它就会把这两个词归纳到一块去，虽然它并不理解团结和力量之间的逻辑关系。

而人类自然语言的学习和推演，从一开始就包含了**语词统计和逻辑推理**的双重属性。这是因为人说话，目的就是为了交流，为了“社会化”。如果失去社会化的需要，人从一开始就不会获得语言。

所以人类幼崽怎么学说话？

一方面，通过观察倾听父母不断重复的发音、节奏、神态、动作，出现频率越高的语词小孩越早学会。另一方面，是通过在符号之间建立逻辑关系，比如“妈妈”—“乳房”，只要喊“neinei”，肯定有饭吃。

**“学习语词”和“学习逻辑”在人类获得语言的过程中几乎是同时进行的。人类学习语言的本质就是【社会化】。**

这是大语言模型AI迄今为止最大的欠缺。

AI没有“社会化”需求，所以它和你进行的“聊天”，是“信息交换”，不是“语言交流”。

所以它不管你的问题有没有逻辑，你说“天上掉猫”，它会在数据库里找这两个词一起出现的词频。它也不管回答符不符合伦理，“如何自杀”，在没有人为干预的情况下，它不会主动询问，“你还好吗”。

它的学习方式是极其“原始”的。它只负责在既有语料数据中采摘符合算法指令的相关信息，但它不负责判断、纠正其中的错误、荒谬、偏见和误会。如果给它一个都是阴谋论的数据库，它拿出来的回答就都是阴谋论，对它来说这没有什么不对。

它可以精准总结一篇量子力学的论文，但无法理解一岁孩子的“neinei”，也无法处理毫无逻辑的指令，比如“北京到纽约有几根脚趾头”，它无法对你的幽默做出回应。

它最漂亮的回答，是针对那些不需要与人社交、互动的问题，依靠巨大的数据库和强大的算力，可以在互联网上浩如烟海的数据中迅速找到与之最匹配的答案。

所以，要说大语言模型可能取代什么工作，那就是无需社会化，不直接面向人，不涉及与人交流，只涉及工作对工作（work2work）的任务。

“**W2W**”，就是在网络上采集大量信息、知识、数据，经过高速和复杂的算法运作，转换成为用户所需的内容，比如生成一篇文献综述、制作一个课程大纲、编写一个产品使用说明。

“**W2P**”（work2people），就是直接服务于人的工作，它可以采用W2W产生的内容，但最终必须依赖社会化经验做出逻辑判断和伦理常识，将此内容deliver给终端客户。

比如做科研，可以借助W2W找到研究缺口，但还要依赖W2P判断科研方向的伦理属性和社会风险。说服伦理委员会、研究基金组织和社会相关组织还是需要人的介入和判断。

还比如做项目，可以借助W2W做前期市场调研，帮你写好proposal，做好ppt，但这些东西如果大家都可以借助W2W就没啥优势。接下来怎么present，怎么deliver，怎么让人信服，怎么获得客户的认可和交托才是成败的关键。

总而言之，这个阶段的AI所能取代的，多半是社会化程度低W2W这样的工作。而AI难以取代的，是需要建立人与人之间的关系、信任和依赖的工作。

其实就算你每天做的是W2W的工作，你能够比其他人多一点personal touch，多迈出一步，了解work背后的people。因为归根到底所有工作都是为了人服务，如果你能看到工作服务的对象，找到更好服务这些人群的方法，那你到最后也是不可替代的。

所以真正不可替代的工作，既不是个人的，也不是AI的，而是**人的社会化与AI的资源化高度结合的工作**。你得知道如何指导和调动AI的逻辑推理和判断，那么首先你自己至少就得比AI更有逻辑性、更能共情、更具备伦理常识。

不具备这些那和AI也没啥区别了，在信息的获取和整合上又毫无优势，那么当使用AI的成本足够低的时候，最先被替换的，一定就是这些和AI没什么区别的人。
